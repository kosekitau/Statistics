{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRsDkoFT_xO5"
      },
      "outputs": [],
      "source": [
        "!pip install japanize-matplotlib\n",
        "!pip install ginza==4.0.6 ja-ginza==4.0.0\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import arviz as az\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "tfb = tfp.bijectors\n",
        "tfd = tfp.distributions\n",
        "\n",
        "sns.reset_defaults()\n",
        "sns.set_context(context='talk', font_scale=0.8)\n",
        "colors = sns.color_palette('tab10')\n",
        "japanize_matplotlib.japanize()\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/projects/instagram/\")\n",
        "\n",
        "import spacy\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data/instagram.csv\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vKawNrZ3__Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# クリーン用の関数\n",
        "def clean_text(text, name2variants=None):\n",
        "    text = BeautifulSoup(text, \"html.parser\") # webページリンクの削除\n",
        "    text = text.get_text(strip=True)\n",
        "    text = re.sub(\"\\n\", \"\", text) # 改行消す\n",
        "    text = text.lower() # 全部小文字へ\n",
        "    text = re.sub(\"w+\", \"w\", text) # 草を一つにする\n",
        "    text = re.sub(\":.*:\", \"\", text) # 絵文字の消去\n",
        "    text = re.sub(\"　\", \" \", text) # 全角空白を半角に\n",
        "    text = re.sub(\"```.*```\", \"\", text) # /code blockは消す\n",
        "    text = re.sub(r'#\\S+', '', text) # ハッシュタグの削除\n",
        "    if name2variants != None: # 表記揺れの処理\n",
        "        for k, v in name2variants.items():\n",
        "            text = re.sub(\"|\".join(v), k, text)\n",
        "    if len(text) == 0: # 0文字になったらdropnaさせる\n",
        "        return np.nan\n",
        "    return text\n",
        "\n",
        "data = df[\"caption\"].apply(clean_text).dropna().tolist()\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAmG8cjfBF5j",
        "outputId": "11526af7-585c-4c0c-f158-33b299a6b7b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "include_pos = (\"NOUN\") # 名詞だけ\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "results = []\n",
        "words = []\n",
        "for doc in nlp.pipe(data):\n",
        "    # include_posに入ってる品詞ワードだけ取り出し\n",
        "    result = [token.lemma_ for token in doc if (token.pos_ in include_pos)]\n",
        "    if len(result)!=0:\n",
        "        results.append(result)\n",
        "        words.extend(result)"
      ],
      "metadata": {
        "id": "xa4wqCYtFwKG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter(words)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "ws, cs = zip(*sorted_word_counts)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "sns.barplot(x=list(ws), y=list(cs), ax=ax)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kxd5bp3KLnor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TN = 1\n",
        "LENGTH_DOCUMENT = 5 # 一定単語数のdocumentは分析から外す\n",
        "\n",
        "# 出現頻度がTN回以上の単語をフィルタリング\n",
        "filtered_words = [w for w, c in word_counts.items() if c > TN]\n",
        "\n",
        "# 辞書の作成\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "for word in filtered_words:\n",
        "    if word not in word2id:\n",
        "        idx = len(word2id)\n",
        "        word2id[word] = idx\n",
        "        id2word[idx] = word\n",
        "\n",
        "\n",
        "# 単語をidに変換\n",
        "documents = []\n",
        "for i in range(len(results)):\n",
        "    # 各投稿を一定出現頻度の単語で表現\n",
        "    document = [word2id[w] for w in results[i] if w in word2id]\n",
        "    if len(document) > LENGTH_DOCUMENT:\n",
        "        documents.append(document)"
      ],
      "metadata": {
        "id": "3asjy8LjHJHx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 2 # トピック数\n",
        "V = len(word2id) # ボキャブラリ数\n",
        "M = len(documents) # 文書数\n",
        "N = [len(d) for d in documents]\n",
        "\n",
        "print(K, V, M)\n",
        "print(N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elemBYCxZww0",
        "outputId": "ba671195-7f6f-4458-c9f3-c3fc9e31c178"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 42 7\n",
            "[10, 13, 26, 13, 11, 10, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Root = tfd.JointDistributionCoroutine.Root\n",
        "def lda_model():\n",
        "    # 文書におけるトピックの分布\n",
        "    # 各文書がどのトピックに属するかはディリクレ分布に従う\n",
        "    theta = yield Root(tfd.Independent(\n",
        "        tfd.Dirichlet(concentration=tf.ones([M, K])),\n",
        "        reinterpreted_batch_ndims=1,\n",
        "        name='theta')) # event shape: M, K\n",
        "\n",
        "    # トピックにおける単語の分布\n",
        "    # 各トピックから生成される単語はディリクレ分布に従う\n",
        "    phi = yield Root(tfd.Independent(\n",
        "        tfd.Dirichlet(concentration=tf.ones([K, V])),\n",
        "        reinterpreted_batch_ndims=1,\n",
        "        name='phi')) # event shape: K, V\n",
        "\n",
        "    for m in range(M):\n",
        "        # 文書mについて\n",
        "        # 観測される単語の分布\n",
        "        y = yield tfd.Sample(\n",
        "            # トピック割り当てzについて周辺化したモデル\n",
        "            tfd.MixtureSameFamily(\n",
        "                # 文書mに含まれる各単語がどのトピックに含まれるかがパラメタthetaのカテゴリカル分布に従う\n",
        "                mixture_distribution=tfd.Categorical(probs=theta[..., m, :]), # カテゴリ数：K\n",
        "                # その単語が含まれるトピックが決まるので、そのトピックの単語のカテゴリカル分布から単語が決まる\n",
        "                components_distribution=tfd.Categorical(probs=phi)), # カテゴリ数：V\n",
        "            sample_shape=N[m],\n",
        "            name=f'y_{m}') # event shape: n, カテゴリ数： V\n",
        "\n",
        "lda = tfd.JointDistributionCoroutine(lda_model)\n",
        "\n",
        "def target_log_prob_fn(theta, phi):\n",
        "    return lda.log_prob(theta, phi, *documents)"
      ],
      "metadata": {
        "id": "90wFkH5IDeyk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_results = 1000\n",
        "num_burnin_steps = 500\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# パラメータの初期値\n",
        "initial_state = [\n",
        "    tf.fill([M, K], value=1/K, name='theta'),\n",
        "    tf.fill([K, V], value=1/V, name='phi')\n",
        "]\n",
        "\n",
        "# パラメータの制約に合わせた変数変換\n",
        "unconstraining_bijectors = [\n",
        "    tfb.SoftmaxCentered(),\n",
        "    tfb.SoftmaxCentered(),\n",
        "]"
      ],
      "metadata": {
        "id": "6RuH0DpfDuee"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# HMC法によるサンプリング用の関数\n",
        "@tf.function(autograph=False)\n",
        "def sample():\n",
        "  return tfp.mcmc.sample_chain(\n",
        "    num_results=num_results,\n",
        "    num_burnin_steps=num_burnin_steps,\n",
        "    current_state=initial_state,\n",
        "    # HMCのステップサイズを自動的に調整\n",
        "    kernel=tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "        # Bijectorを利用して変数の制約に対処\n",
        "        tfp.mcmc.TransformedTransitionKernel(\n",
        "            # HMC法\n",
        "            inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
        "                target_log_prob_fn=target_log_prob_fn,\n",
        "                 step_size=0.1,\n",
        "                 num_leapfrog_steps=5),\n",
        "            bijector=unconstraining_bijectors),\n",
        "         num_adaptation_steps=400),\n",
        "    trace_fn=lambda _, pkr: pkr.inner_results.inner_results.is_accepted)\n",
        "\n",
        "[theta, phi], is_accepted = sample()\n",
        "\n",
        "print('acceptance rate: {:.1%}'.format(is_accepted.numpy().mean()))"
      ],
      "metadata": {
        "id": "WJCREJfhD6-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(theta.shape, phi.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BuBFE_UZOYG",
        "outputId": "dd015226-ed3e-4cd1-891b-e23ab044f268"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 7, 2) (1000, 2, 42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_trace(states, var_name, chain_dim=None):\n",
        "    if chain_dim is None:\n",
        "        # chainが１つだと明示するためaxisを追加\n",
        "        trace = {k: v[tf.newaxis].numpy() for k, v in zip(var_name, states)}\n",
        "    else:\n",
        "        # axis0がchainの次元になるようにする\n",
        "        trace = {k: np.swapaxes(v.numpy(), chain_dim, 0) for k, v in zip(var_name, states)}\n",
        "    # from_tfpもあるが、実行するとeager_executionがオフにされてしまうなど現状使いづらいので、from_dictを用いている\n",
        "    return az.from_dict(trace)\n",
        "\n",
        "trace = format_trace([theta, phi], ['theta', 'phi'])\n",
        "az.plot_trace(trace)\n",
        "plt.tight_layout();"
      ],
      "metadata": {
        "id": "tIFqJXzBE0R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (1 - a) x 100% のサンプルが入る区間を求める\n",
        "a = 0.1\n",
        "# phiは各トピックにおける単語の分布\n",
        "# サンプルを1000こ取ってるのでその5%、50%、95%点を求める\n",
        "lwr, med, upr = np.quantile(phi, [a / 2, 0.5, 1 - a / 2], axis=0)\n",
        "\n",
        "# 描画用\n",
        "xticks = range(V)\n",
        "vocabs = id2word.values()\n",
        "\n",
        "fig, axes = plt.subplots(K, 1, sharex=True, sharey=True, figsize=(12, 3*K))\n",
        "\n",
        "for i in range(K):\n",
        "    ax = axes[i]\n",
        "    ax.scatter(range(V), med[i], color=colors[0], marker='s', label='pred')\n",
        "    ax.vlines(range(V), lwr[i], upr[i], color=colors[0], label=f'{1-a:.0%} HDI')\n",
        "    #ax.scatter(range(V), word_dist[i], color=colors[1], marker='x', label='truth')\n",
        "    if i == K - 1:\n",
        "        ax.set_xlabel('word')\n",
        "    ax.set_ylabel('probability')\n",
        "    ax.set_xticks(xticks)\n",
        "    ax.set_xticklabels(vocabs, rotation=90)\n",
        "    ax.set_title(f'topic {i}')\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='center left', bbox_to_anchor=[1.0, 0.5])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "dUVlkZXpGWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各文書がどのトピックに含まれるか\n",
        "a = 0.1\n",
        "lwr, med, upr = np.quantile(theta, [a / 2, 0.5, 1 - a / 2], axis=0)\n",
        "\n",
        "ncol = 5\n",
        "nrow = round(np.ceil(M/ncol))\n",
        "fig, axes = plt.subplots(nrow, ncol, sharex=True, sharey=True, figsize=(ncol*3, nrow*2))\n",
        "\n",
        "for i in range(M):\n",
        "    ax = axes.ravel()[i]\n",
        "    ax.scatter(range(K), med[i], color=colors[0], marker='s', label='pred')\n",
        "    ax.vlines(range(K), lwr[i], upr[i], color=colors[0], label=f'{1-a:.0%} HDI')\n",
        "    #ax.scatter(range(K), topic_dist[i], color=colors[1], marker='x', label='truth')\n",
        "    # ax.legend()\n",
        "    ax.set_xticks(range(K))\n",
        "    if i >= ncol * (nrow-1):\n",
        "        ax.set_xlabel('topic')\n",
        "    if not i % ncol:\n",
        "        ax.set_ylabel('probability')\n",
        "    ax.set_title(f'document {i}')\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='center left', bbox_to_anchor=[1.0, 0.5])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7h8jt8GGHSGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSrbn9IYQ7I7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}